{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, torch\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('/home/yungshun/workspace/py3/pytorch-mnist-dnn-classifier/datasets/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build a network with `nn.Sequential` here. Only difference from the last part is we're not actually using softmax on the output, but instead just using the raw output from the last layer. This is because the output from softmax is a probability distribution. Often, the output will have values really close to zero or really close to one. Due to [inaccuracies with representing numbers as floating points](https://docs.python.org/3/tutorial/floatingpoint.html), computations with a softmax output can lose accuracy and become unstable. To get around this, we'll use the raw output, called the **logits**, to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "    ('relu2', nn.ReLU()),\n",
    "    ('logits', nn.Linear(hidden_sizes[1], output_size))\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function. In PyTorch, you'll usually see this as criterion. \n",
    "Here we're using softmax output, so we want to use `criterion = nn.CrossEntropyLoss()` \n",
    "as our loss. Later when training, you use `loss = criterion(output, targets)` \n",
    "to calculate the actual loss.\n",
    "\n",
    "We also need to define the optimizer we're using, SGD or Adam, or something along \n",
    "those lines. Here I'll just use SGD with `torch.optim.SGD`, passing in the network \n",
    "parameters and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general process with PyTorch:\n",
    "1. Make a forward pass through the network to get the logits \n",
    "2. Use the logits to calculate the loss\n",
    "3. Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "4. Take a step with the optimizer to update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights - Parameter containing:\n",
      "tensor([[-0.0163, -0.0299, -0.0349,  ...,  0.0347, -0.0252, -0.0058],\n",
      "        [-0.0186, -0.0143,  0.0088,  ..., -0.0062, -0.0316,  0.0136],\n",
      "        [ 0.0213,  0.0053, -0.0127,  ...,  0.0154, -0.0284, -0.0277],\n",
      "        ...,\n",
      "        [ 0.0081,  0.0004, -0.0207,  ..., -0.0252,  0.0056, -0.0110],\n",
      "        [ 0.0112,  0.0079,  0.0223,  ...,  0.0086,  0.0063, -0.0308],\n",
      "        [-0.0145, -0.0104,  0.0213,  ..., -0.0227, -0.0254,  0.0206]],\n",
      "       requires_grad=True)\n",
      "Gradient - tensor([[-0.0012, -0.0012, -0.0012,  ..., -0.0012, -0.0012, -0.0012],\n",
      "        [-0.0010, -0.0010, -0.0010,  ..., -0.0010, -0.0010, -0.0010],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002],\n",
      "        [ 0.0001,  0.0001,  0.0001,  ...,  0.0001,  0.0001,  0.0001],\n",
      "        [-0.0005, -0.0005, -0.0005,  ..., -0.0005, -0.0005, -0.0005]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights -', model.fc1.weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model.forward(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model.fc1.weight.grad)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights - Parameter containing:\n",
      "tensor([[-0.0163, -0.0299, -0.0349,  ...,  0.0347, -0.0252, -0.0058],\n",
      "        [-0.0186, -0.0143,  0.0088,  ..., -0.0062, -0.0316,  0.0136],\n",
      "        [ 0.0213,  0.0053, -0.0127,  ...,  0.0154, -0.0284, -0.0277],\n",
      "        ...,\n",
      "        [ 0.0081,  0.0004, -0.0207,  ..., -0.0252,  0.0056, -0.0110],\n",
      "        [ 0.0112,  0.0079,  0.0223,  ...,  0.0086,  0.0063, -0.0308],\n",
      "        [-0.0145, -0.0104,  0.0213,  ..., -0.0227, -0.0254,  0.0206]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Updated weights -', model.fc1.weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
